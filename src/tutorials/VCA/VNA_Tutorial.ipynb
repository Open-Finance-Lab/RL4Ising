{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99676ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "from math import sqrt\n",
    "\n",
    "try:\n",
    "    from config import config\n",
    "    from DilatedRNN import DilatedRNNWavefunction\n",
    "    from utils import Fullyconnected_localenergies, Fullyconnected_diagonal_matrixelements\n",
    "    from vca import vca_solver\n",
    "    print(\"Successfully imported local modules.\")\n",
    "except ImportError:\n",
    "    print(\"Local .py files not found. Please ensure they are in the same directory or paste the classes below.\")\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b676f9fb",
   "metadata": {},
   "source": [
    "# Section 1: Ising Model as Energy Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c29caa",
   "metadata": {},
   "source": [
    "## 1.1 What is the Ising Model?\n",
    "The Ising model was originally developed in statistical physics to describe the **ferromagnetism** of solid materials. It assumes that a system consists of a collection of **spins** arranged on a lattice, where each spin can only take one of two discrete values:\n",
    "* $s_i = +1$ (Spin Up)\n",
    "* $s_i = -1$ (Spin Down)\n",
    "\n",
    "In this implementation, the spins are mapped to binary values $s \\in \\{0, 1\\}$ to be compatible with neural network processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c98273",
   "metadata": {},
   "source": [
    "## 1.2 Mathematical Nature & The Hamiltonian\n",
    "The **Hamiltonian** is a function in physics that represents the total energy of a system. For a fully connected Ising model, the energy function $E(\\mathbf{s})$ is defined as:\n",
    "\n",
    "$$E(\\mathbf{s}) = -\\sum_{i < j} J_{ij} \\sigma_i \\sigma_j - \\sum_i h_i \\sigma_i$$\n",
    "\n",
    "* **$J_{ij}$ (Interaction Term)**: Defines the coupling strength between spins $i$ and $j$.\n",
    "    * If $J_{ij} > 0$: Spins tend to align in the same direction (Ferromagnetic).\n",
    "    * If $J_{ij} < 0$: Spins tend to align in opposite directions (Anti-ferromagnetic).\n",
    "* **$h_i$ (External Field)**: Represents the influence of an external magnetic field or bias on an individual spin. In this code, a transverse field `Bx` is introduced to add non-diagonal perturbations.\n",
    "* **Code Example**: In `utils.py`, the `Jz` matrix stores all interaction weights, and the function `Fullyconnected_diagonal_matrixelements` calculates the classical energy based on these weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a03bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fullyconnected_diagonal_matrixelements(Jz, samples):\n",
    "    numsamples = samples.shape[0]\n",
    "    N = samples.shape[1]\n",
    "    energies = np.zeros((numsamples), dtype = np.float64)\n",
    "\n",
    "    for i in range(N-1):\n",
    "      values = np.expand_dims(samples[:,i], axis = -1)+samples[:,i+1:]\n",
    "      valuesT = np.copy(values)\n",
    "      valuesT[values==2] = +1 #If both spins are up\n",
    "      valuesT[values==0] = +1 #If both spins are down\n",
    "      valuesT[values==1] = -1 #If they are opposite\n",
    "\n",
    "      energies += np.sum(valuesT*(-Jz[i,i+1:]), axis = 1)\n",
    "\n",
    "    return energies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50879217",
   "metadata": {},
   "source": [
    "## 1.3 Problem Type: Combinatorial Optimization\n",
    "From a computer science perspective, the Ising model is a classic **Combinatorial Optimization Problem**:\n",
    "* **Goal**: To find a specific arrangement among $2^N$ possible discrete configurations that minimizes the total energy of the system.\n",
    "* **Applications**: Many famous NP-hard problems, such as the **Max-Cut Problem**, **Traveling Salesperson Problem (TSP)**, or **Graph Coloring**, can be exactly mapped onto the energy minimization of an Ising model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad844f1b",
   "metadata": {},
   "source": [
    "## 1.4 The Essence of NP-hardness\n",
    "Finding the lowest energy state (the **Ground State**) of an Ising model is extremely difficult for the following reasons:\n",
    "\n",
    "1.  **Exponential State Space**: For $N$ spins, there are $2^N$ possible states. Brute-force search becomes impossible as $N$ grows.\n",
    "2.  **Frustration**: When the distribution of $J_{ij}$ is complex (as in Spin Glass models), the system cannot satisfy all coupling terms simultaneously leading towards numerous equivalent ground states.\n",
    "3.  **Complex Energy Landscape**: Frustration leads to a highly non-convex energy landscape filled with numerous **Local Minima**.\n",
    "4.  **Mathematical Bottleneck**: Due to these high energy barriers, traditional optimization algorithms easily get stuck in local optima, failing to find the global optimum. This is the core of its NP-hardness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e4c2d",
   "metadata": {},
   "source": [
    "# Section 2:Simulated Annealing as a Classical Heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435161b",
   "metadata": {},
   "source": [
    "## 2.1 What is Simulated Annealing\n",
    "Simulated Annealing (SA) is a classical **probabilistic optimization algorithm** designed to find low-energy (or high-quality) solutions in large, discrete, and non-convex search spaces. It was originally inspired by the physical annealing process in metallurgy, but in optimization it should be understood primarily as a **randomized search heuristic** rather than a physical simulation.\n",
    "\n",
    "Formally, simulated annealing defines a **discrete-time, inhomogeneous Markov chain** over the solution space. Starting from an initial solution, the algorithm repeatedly proposes local moves within a predefined neighborhood structure. If a proposed move improves the objective value, it is always accepted. Otherwise, the move may still be accepted with a probability that depends on both the **increase in cost** and a **temperature parameter** that decreases over time.\n",
    "\n",
    "The key feature of simulated annealing is its ability to **accept worse solutions during the early stages of the search**, which allows the algorithm to escape local minima that commonly trap greedy or purely descent-based methods. As the temperature is gradually lowered according to a cooling schedule, the algorithm becomes increasingly conservative and focuses on refining high-quality solutions.\n",
    "\n",
    "From a theoretical perspective, the acceptance rule of simulated annealing is closely related to the **Boltzmann (Gibbs) distribution**. When the temperature is held fixed, the induced Markov chain admits a Boltzmann distribution as its invariant distribution. As the temperature approaches zero, this distribution concentrates on the set of global minima of the objective function. A rigorous analysis of this viewpoint and the convergence properties of simulated annealing can be found in the classical work of Bertsimas and Tsitsiklis (1993)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e86dff1",
   "metadata": {},
   "source": [
    "## 2.2 How Simulated Annealing works\n",
    "\n",
    "This section reconstructs the simulated annealing procedure by following the basic elements defined in the classical formulation of Bertsimas and Tsitsiklis (1993).\n",
    "\n",
    "### Solution Space and Objective\n",
    "\n",
    "Simulated annealing operates on a **finite set of feasible solutions**, denoted by $S$.  \n",
    "Each solution represents a valid configuration of the problem (e.g., a spin configuration in the Ising model).\n",
    "\n",
    "An objective (or energy) function $J$ is defined on $S$, assigning a real-valued cost to each solution.  \n",
    "The goal of the algorithm is to find solutions with **minimal objective value**, ideally belonging to the set of global minima.\n",
    "\n",
    "### Neighborhood Structure\n",
    "\n",
    "For each solution $i \\in S$, a **neighborhood set** $S(i)$ is defined.  \n",
    "This set specifies which solutions can be reached from $i$ through a single local modification.\n",
    "\n",
    "The neighborhood structure determines how the algorithm explores the solution space and encodes the notion of a *local move*.\n",
    "\n",
    "### Randomized Move Proposal\n",
    "\n",
    "At each iteration, the algorithm selects a candidate solution from the neighborhood of the current solution.  \n",
    "This selection is randomized according to a fixed probability rule, ensuring that the search does not follow a deterministic path.\n",
    "\n",
    "This randomness is essential for exploration and allows the algorithm to visit different regions of the solution space.\n",
    "\n",
    "### Temperature and Acceptance Behavior\n",
    "\n",
    "The algorithm maintains a **temperature parameter** that changes over time according to a predefined cooling schedule.  \n",
    "The temperature controls how willing the algorithm is to accept worse solutions.\n",
    "\n",
    "- At high temperature, the algorithm is more exploratory and may accept worse solutions frequently.\n",
    "- At low temperature, the algorithm becomes more conservative and favors improvements.\n",
    "\n",
    "This mechanism enables simulated annealing to escape local minima early in the search and gradually refine solutions as the temperature decreases.\n",
    "\n",
    "### State Evolution\n",
    "\n",
    "Starting from an initial solution, the algorithm repeatedly:\n",
    "1. Proposes a neighboring solution at random,\n",
    "2. Evaluates its objective value,\n",
    "3. Decides whether to accept or reject the move based on the current temperature.\n",
    "\n",
    "Through this process, the algorithm generates a sequence of solutions that gradually concentrates on low-energy regions of the solution space.\n",
    "\n",
    "---\n",
    "\n",
    "**Remark.**  \n",
    "From a theoretical perspective, this iterative process defines a time-inhomogeneous Markov chain over the solution space. When the temperature is held constant, the long-run behavior of the algorithm favors low-cost solutions, providing the foundation for simulated annealing as a global optimization heuristic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1339f983",
   "metadata": {},
   "source": [
    "## 2.2.1 Boltzmann Distribution (Intuition)\n",
    "\n",
    "This section provides additional intuition for the **acceptance behavior** described in the previous section.  \n",
    "Rather than introducing a new algorithmic step, it explains *why* simulated annealing uses a temperature-dependent probabilistic acceptance rule.\n",
    "\n",
    "### Preference for Low-Energy Solutions\n",
    "\n",
    "The Boltzmann distribution expresses a simple and intuitive idea:\n",
    "\n",
    "**solutions with lower energy (or lower cost) are more likely than solutions with higher energy**.\n",
    "\n",
    "This preference is controlled by a temperature parameter:\n",
    "- At **high temperature**, differences in energy matter less, and many solutions are treated similarly.\n",
    "- At **low temperature**, low-energy solutions are strongly preferred over higher-energy ones.\n",
    "\n",
    "This temperature-dependent preference mirrors how physical systems behave under thermal fluctuations.\n",
    "\n",
    "### Connection to Simulated Annealing\n",
    "\n",
    "In simulated annealing, the Boltzmann distribution is not used for explicit sampling.  \n",
    "Instead, it provides a **conceptual explanation** for the acceptance rule used during the search.\n",
    "\n",
    "- Better solutions are always accepted.\n",
    "- Worse solutions may still be accepted, especially when the temperature is high.\n",
    "\n",
    "This behavior allows the algorithm to move uphill in the objective landscape early in the search, helping it escape local minima.\n",
    "\n",
    "As the temperature decreases, the probability of accepting worse solutions diminishes, and the algorithm gradually focuses on improving the current solution.\n",
    "\n",
    "### Boltzmann Form (Interpretation)\n",
    "\n",
    "When the temperature is held fixed, the search process favors solutions according to a Boltzmann-like preference:\n",
    "\n",
    "$$\n",
    "\\pi(i) \\propto \\exp\\!\\left(-\\frac{J(i)}{T}\\right),\n",
    "$$\n",
    "\n",
    "where $J(i)$ denotes the objective (or energy) of solution $i$, and $T$ is the temperature.\n",
    "\n",
    "This distribution is **not computed explicitly** during the algorithm.  \n",
    "Rather, the local acceptance rule of simulated annealing is designed so that the overall search behavior is consistent with this preference.\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "**The Boltzmann distribution explains why simulated annealing behaves the way it does.**  \n",
    "It provides a theoretical lens for understanding how temperature controls exploration and exploitation, while the algorithm itself remains a simple, local, randomized search procedure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c5f294",
   "metadata": {},
   "source": [
    "# Section 3: From Simulated Annealing to Learing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3454a3",
   "metadata": {},
   "source": [
    "## 3.1 Limitations of Simulated Annealing\n",
    "Simulated annealing is a well-established classical heuristic, but it exhibits several limitations when applied to large-scale and highly structured optimization problems.\n",
    "\n",
    "The search behavior of simulated annealing is **manually designed**.\n",
    "Key components such as neighborhood moves, acceptance rules, and temperature schedules are specified a priori and remain fixed, limiting the algorithm’s ability to adapt to instance-specific structure.\n",
    "\n",
    "Moreover, simulated annealing does not **accumulate or reuse experience**.\n",
    "Each run is independent, and information obtained during previous searches is not leveraged to improve future performance.\n",
    "\n",
    "In rugged energy landscapes such as spin glass systems, the resulting random walk may spend a substantial amount of time exploring suboptimal regions before reaching low-energy configurations.\n",
    "\n",
    "These limitations motivate learning-based approaches, which aim not to eliminate randomness, but to **learn heuristics that more effectively guide the search process**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e7ab7",
   "metadata": {},
   "source": [
    "## 3.2 Why learning methods make sense\n",
    "\n",
    "\n",
    "Compared to classical simulated annealing, learning-based methods offer several practical advantages when applied to large-scale NP-hard optimization problems. Simulated annealing relies on fixed, hand-designed components such as neighborhood moves and temperature schedules, which remain unchanged across different problem instances. In contrast, learning-based methods can adapt the search strategy by extracting patterns from data or repeated search experience, allowing the heuristic to better align with the underlying structure of the problem. \n",
    "\n",
    "Moreover, while simulated annealing treats each run independently, learning-based approaches can amortize computational cost across multiple instances by reusing learned knowledge, enabling faster generation of high-quality candidate solutions at inference time. This shift from instance-specific random exploration to experience-driven guidance is particularly beneficial in complex energy landscapes, where naive random walks may suffer from slow exploration and high computational cost. Importantly, learning-based methods do not remove randomness or circumvent the NP-hard nature of the problem, but they can significantly improve practical efficiency by guiding randomized search toward more promising regions of the solution space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382bf9a2",
   "metadata": {},
   "source": [
    "# Section 4: VNA as Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841dd0cf",
   "metadata": {},
   "source": [
    "Here is the link to VNA Github: https://github.com/VectorInstitute/VariationalNeuralAnnealing/tree/main?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfaf27c",
   "metadata": {},
   "source": [
    "## 4.1 Motivation\n",
    "\n",
    "Simulated annealing solves optimization problems by approximately sampling a sequence of Boltzmann distributions, but its effectiveness is often limited by slow sampling dynamics in rugged or glassy energy landscapes. In practical, finite-time runs, the system deviates from equilibrium, leading to suboptimal solutions.\n",
    "\n",
    "Variational Neural Annealing addresses this limitation by replacing the exact Boltzmann distribution with a flexible, parameterized model that can be efficiently sampled. Instead of annealing the physical system itself, VNA anneals a learnable probability distribution whose parameters are optimized using variational principles and energy feedback. Modern autoregressive neural networks provide an ideal parameterization, as they allow exact and fast sampling even when representing complex, highly structured landscapes. This shift from trajectory-based sampling to distribution-based learning enables more efficient exploration of low-energy configurations under limited computational budgets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e94747",
   "metadata": {},
   "source": [
    "## 4.2 Algorithm Logic\n",
    "\n",
    "This section provides a step-by-step walkthrough of a learning-based annealing method for Ising optimization.  \n",
    "Each conceptual step is explicitly linked to a corresponding function in the implementation, allowing readers to connect algorithmic ideas with concrete code components.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fbd61b",
   "metadata": {},
   "source": [
    "### 1. Load an Ising Instance\n",
    "\n",
    "**Relevant functions:**  \n",
    "- `config.read_graph(graph_path)`  \n",
    "- `config.__init__()`\n",
    "\n",
    "We begin by loading an Ising problem instance from file. The graph structure and coupling strengths are encoded in a dense interaction matrix , which defines the Ising Hamiltonian. This step specifies the optimization problem and is independent of the solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac15adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the configuration with a specific Ising instance\n",
    "instance_path = \"dataset/EA/EA_10x10/10x10_uniform_seed1.txt\" \n",
    "cfg = config(instance_path, seed=42)\n",
    "\n",
    "# Verify the system scale and problem dimensions\n",
    "print(f\"System Size (N): {cfg.N}\") # Number of spins (sites)\n",
    "print(f\"J-Matrix Shape: {cfg.Jz.shape}\") # Should be an (N x N) matrix\n",
    "\n",
    "# Physical Consistency Check: The interaction J_ij must equal J_ji (Symmetry)\n",
    "# In physics, the coupling between spin i and spin j is mutual.\n",
    "is_symmetric = (cfg.Jz == cfg.Jz.T).all()\n",
    "print(f\"Is the J-matrix symmetric? {is_symmetric}\")\n",
    "\n",
    "# Inspect a sub-section of the interaction matrix\n",
    "# This represents a local 'snapshot' of the couplings between the first 5 spins.\n",
    "print(\"\\nLocal Sampling of J-Matrix (Top 5x5):\")\n",
    "print(cfg.Jz[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334658f9",
   "metadata": {},
   "source": [
    "### 2. Define the Neural Sampler\n",
    "\n",
    "**Relevant class:**  \n",
    "- `DilatedRNNWavefunction`\n",
    "\n",
    "The neural sampler parameterizes a probability distribution over spin configurations. We use an autoregressive recurrent neural network to model the joint distribution of spins, enabling exact and efficient sampling without Markov chain dynamics. The network serves as a learnable heuristic that guides the randomized search process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ee6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each RNN layer uses the same number of hidden units.\n",
    "# cfg.num_layers controls how many dilated layers are used.\n",
    "# cfg.num_units controls the capacity of each RNN cell.\n",
    "units = [cfg.num_units] * cfg.num_layers\n",
    "# Instantiate the autoregressive neural sampler\n",
    "rnn_sampler = DilatedRNNWavefunction(\n",
    "    systemsize=cfg.N,                 # Number of spins in the Ising model\n",
    "    units=units,                      # Hidden units per RNN layer\n",
    "    layers=cfg.num_layers,            # Number of dilated RNN layers\n",
    "    activation=cfg.activation_function,\n",
    "    seed=cfg.seed\n",
    ")\n",
    "\n",
    "print(\"Number of spins (N):\", cfg.N)\n",
    "print(\"Number of RNN layers:\", cfg.num_layers)\n",
    "print(\"Hidden units per layer:\", cfg.num_units)\n",
    "print(\"RNN cells per layer:\", len(rnn_sampler.rnn[0]))\n",
    "print(\"Total number of RNN cells:\", cfg.num_layers * cfg.N)\n",
    "#Each layer contains one RNN cell per spin position.  \n",
    "#This design allows the model to capture both short-range and long-range dependencies between spins through dilated connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8ac4fd",
   "metadata": {},
   "source": [
    "### 3. Sampling Spin Configurations\n",
    "\n",
    "**Relevant method:**  \n",
    "- `DilatedRNNWavefunction.sample(numsamples, inputdim)`\n",
    "\n",
    "At each iteration, the model generates a batch of complete spin configurations by sampling from the learned distribution. Each sample corresponds to a candidate solution to the Ising optimization problem. Randomness is preserved, but the sampling is biased by the learned model parameters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f1722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of configurations to sample at each iteration\n",
    "num_samples = cfg.numsamples\n",
    "\n",
    "# IMPORTANT:\n",
    "# The following call builds the sampling operations in the\n",
    "# TensorFlow 1.x computation graph.\n",
    "# `samples` and `log_probs` are symbolic Tensors, not concrete values yet.\n",
    "samples, log_probs = rnn_sampler.sample(\n",
    "    numsamples=num_samples,\n",
    "    inputdim=2\n",
    ")\n",
    "\n",
    "print(\"Symbolic sample tensor shape:\", samples.shape)\n",
    "print(\"Symbolic log-probability tensor shape:\", log_probs.shape)\n",
    "\n",
    "# In TensorFlow 1.x, tensors must be explicitly evaluated\n",
    "# inside a Session to obtain NumPy arrays.\n",
    "with tf.compat.v1.Session(graph=rnn_sampler.graph) as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    \n",
    "    # Run the sampling operation\n",
    "    samples_val, log_probs_val = sess.run([samples, log_probs])\n",
    "\n",
    "# At this point, `samples_val` and `log_probs_val` are NumPy arrays\n",
    "print(\"Sample shape (NumPy):\", samples_val.shape)\n",
    "print(\"First 5 sampled configurations:\")\n",
    "print(samples_val[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e8646",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Energy Evaluation\n",
    "\n",
    "**Relevant functions:**  \n",
    "- `Fullyconnected_diagonal_matrixelements(Jz, samples)`  \n",
    "- `Fullyconnected_localenergies(...)` (optional)\n",
    "\n",
    "The quality of sampled configurations is evaluated using the Ising Hamiltonian. Energy values provide the only feedback signal used to guide learning. Lower-energy configurations correspond to better approximate solutions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a30414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jz encodes the pairwise couplings of the Ising Hamiltonian\n",
    "Jz = cfg.Jz\n",
    "\n",
    "# Compute classical Ising energies for each sampled configuration\n",
    "energies = Fullyconnected_diagonal_matrixelements(Jz, samples_val)\n",
    "\n",
    "print(\"Energy array shape:\", energies.shape)\n",
    "print(\"Mean energy:\", energies.mean())\n",
    "print(\"Minimum energy:\", energies.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069dbc2e",
   "metadata": {},
   "source": [
    "### 5. Training with Annealing\n",
    "\n",
    "**Relevant function:**  \n",
    "- `vca_solver(config)`\n",
    "\n",
    "Model parameters are updated using energy feedback under a temperature-controlled objective. A warmup phase stabilizes learning, followed by an annealing schedule that gradually shifts the focus from exploration to exploitation. Unlike classical simulated annealing, annealing here regulates the learning objective rather than the sampling dynamics.\n",
    "\n",
    "Training begins with a **warmup phase**, during which the model stabilizes before annealing is applied.  \n",
    "This is followed by an **annealing phase**, where the temperature is gradually reduced. At higher temperatures, the learning objective encourages exploration across diverse configurations. As the temperature decreases, updates increasingly focus on concentrating probability mass around low-energy regions.\n",
    "\n",
    "A key distinction from classical simulated annealing is that annealing in VCA does not control the sampling dynamics. Instead, it regulates the learning objective, acting as a curriculum that guides how the sampling distribution evolves over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c22d43",
   "metadata": {},
   "source": [
    "### 6. Final Sampling for Benchmarking\n",
    "\n",
    "**Relevant code block:**  \n",
    "- Final sampling section in `vca_solver`\n",
    "\n",
    "After the annealing process completes, the trained model is used to generate a large number of spin configurations efficiently. The minimum and average energies of these samples are reported as benchmark metrics, reflecting the practical performance of the learned heuristic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191dcfe9",
   "metadata": {},
   "source": [
    "## 4.3 Execution Cell\n",
    "To run the whole solver in this Notebook, use the following code block. Ensure you have the problem instance file (e.g., `10x10_uniform_seed1.txt`) in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e8f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Path to your problem instance\n",
    "instance_path = \"dataset/EA/EA_10x10/10x10_uniform_seed1.txt\" \n",
    "seed = 0\n",
    "\n",
    "# 2. Initialize configuration\n",
    "vca_config = config(instance_path, seed)\n",
    "\n",
    "# 3. Run the solver\n",
    "# This will output the annealing progress, energy (E), and free energy (F)\n",
    "mean_energies, min_energies = vca_solver(vca_config)\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Minimum Energy Found: {min_energies}\")\n",
    "print(f\"Mean Energy: {mean_energies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f395513c",
   "metadata": {},
   "source": [
    "**Reference**\n",
    "\n",
    "D. Bertsimas and J. N. Tsitsiklis, *Simulated Annealing*, Statistical Science, vol. 8, no. 1, pp. 10–15, 1993.  \n",
    "Available at: https://www.mit.edu/~dbertsim/papers/Optimization/Simulated%20annealing.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vca, TF1.13)",
   "language": "python",
   "name": "vca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
